= Produces and Consumers
include::_attributes.adoc[]

[#producer]
== What are Producers?

Producers publish messages to a topic at the end of a partition.
By default, if a message contains a key, the hashed value of the key is used to decide in which partition the message is stored.
If the key is `null`, then a round-robin algorithm is used to balance the messages across all partitions.

Kafka guarantees that all as long as no new partitions are added, elements with the same key will be stored in the same partition.

If you want you can also implement a custom partitioning strategy to store messages in a specific partition following any kind of business rule.

image::p.png[]

[#consumer]
== What are Consumers?

Each message published to a topic is delivered to a consumer that is subscribed to that topic.

A consumer can read data from any position of the partition, and internally it is stored as a pointer called `offset`.
In most of the cases, a consumer advances its `offset` linearly, but it could be in any order, or starting from any given position.

Each consumer belongs to a consumer group.
A consumer group is a list of consumer instances that ensures the scalability for processing messages and also fault tolerance.

When a consumer group only contains one consumer, this consumer is responsible for processing all messages of all partitions.

The more consumers you add to a group, each consumer will only receive messages from a subset of the partitions.

image::c.png[]

IMPORTANT: If you add more consumers in a consumer group than the number of partitions for a topic, then they will stay in an idle state, without getting any message.

As you can see what is registered to a topic is the _consumer group_ and not the consumer.
The consumer is "subscribed" to a partition.

== Consume and Produce Messages

Before digging into how to produce and consume data from Java, let's see how to use `kcat` to do it.

`kcat` is a great tool for these use cases, and it is the _de-facto_ tool to inspect data in Kafka.

[#consume-kafkacat]
=== Consuming messages with Kafkacat

Open a new terminal, _terminal 1_, and source the environment file to set the environment variables for the Kafka instance bootstrap URL, and the service account ID and secret.

[.console-input]
[source, bash-shell]
----
source $TUTORIAL_HOME/apps/env
----

Run the following command:

[tabs]
====
kcat::
+
--
[.console-input]
[source, bash-shell]
----
kcat -b $BOOTSTRAP_SERVER -X security.protocol=SASL_SSL -X sasl.mechanisms=PLAIN -X sasl.username="$CLIENT_ID" -X sasl.password="$CLIENT_SECRET" -t songs -C -K:
----
--
kcat in Docker::
+
--
[.console-input]
[source, bash-shell]
----
docker run -it --rm edenhill/kcat:1.7.0 kcat -b $BOOTSTRAP_SERVER -X security.protocol=SASL_SSL -X sasl.mechanisms=PLAIN -X sasl.username="$CLIENT_ID" -X sasl.password="$CLIENT_SECRET" -t songs -C -K:
----
--
====

[.console-output]
[source, bash-shell]
----
% Reached end of topic songs [0] at offset 0
----

[#produce-kafkacat]
=== Producing messages with Kafkacat

Open a new terminal, _terminal 2_, and source the environment file to set the environment variables for the Kafka instance bootstrap URL, and the service account ID and secret.

[.console-input]
[source, bash-shell]
----
source $TUTORIAL_HOME/apps/env
----

Create a new file with the following content:

[.console-input]
[source, json]
.initial_songs
----
include::example$initial_songs.json[]
----

Then insert each of this line as a new message.
The number before the `:` is taken as an id.

[tabs]
====
kcat::
+
--
[.console-input]
[source, bash-shell]
----
kcat -b $BOOTSTRAP_SERVER -X security.protocol=SASL_SSL -X sasl.mechanisms=PLAIN -X sasl.username="$CLIENT_ID" -X sasl.password="$CLIENT_SECRET" -t songs -P -l -K: initial_songs
----
--
kcat in Docker::
+
--
[.console-input]
[source, bash-shell]
----
docker run -it --mount type=bind,source="$(pwd)"/initial_songs,destination=/tmp/initial_songs --rm edenhill/kcat:1.7.0 kcat -b $BOOTSTRAP_SERVER -X security.protocol=SASL_SSL -X sasl.mechanisms=PLAIN -X sasl.username="$CLIENT_ID" -X sasl.password="$CLIENT_SECRET" -t songs -P -l -K: /tmp/initial_songs
----
--
====

Switch back to _terminal 1_ where you are running `kcat` as a consumer.
You've defined this at <<Consuming messages with Kafkacat>> section.

Now the output of the terminal has been updated showing the consumed messages:

[.console-output]
[source, bash-shell]
----
1: {"id": 1, "name": "The Ecstasy of Gold", "author":"Ennio Morricone", "op":"ADD"}
2: {"id": 2, "name": "Still Loving you", "author":"Scorpions", "op":"ADD"}
% Reached end of topic songs [0] at offset 2
----

Notice that the `offset` of this consumer has been updated to 2, as the two first records has been consumed from the topic.

[#playingwithoffsets]
=== Playing with Offsets

A Producer always produces content at the end of a topic, meanwhile, a consumer can consume from any offset.
Still on _terminal 1_, stop the `kcat` process by typing kbd:[Ctrl + C].

Then start `kcat` again with the same command:

[tabs]
====
kcat::
+
--
[.console-input]
[source, bash-shell]
----
kcat -b $BOOTSTRAP_SERVER -X security.protocol=SASL_SSL -X sasl.mechanisms=PLAIN -X sasl.username="$CLIENT_ID" -X sasl.password="$CLIENT_SECRET" -t songs -P -l -K:
----
--
kcat in Docker::
+
--
[.console-input]
[source, bash-shell]
----
docker run -it --rm edenhill/kcat:1.7.0 kcat -b $BOOTSTRAP_SERVER -X security.protocol=SASL_SSL -X sasl.mechanisms=PLAIN -X sasl.username="$CLIENT_ID" -X sasl.password="$CLIENT_SECRET" -t songs -P -l -K:
----
--
====

[.console-output]
[source, bash-shell]
----
1: {"id": 1, "name": "The Ecstasy of Gold", "author":"Ennio Morricone", "op":"ADD"}
2: {"id": 2, "name": "Still Loving you", "author":"Scorpions", "op":"ADD"}
% Reached end of topic songs [0] at offset 2
----

Notice that the complete stream is consumed.
This is happening because by default `kcat` is getting all the content from a topic since the beginning of the stream.
But you can change that.

Stop again `kcat` process by typing kbd:[Ctrl + C].

And start again with the following command:

[tabs]
====
kcat::
+
--
[.console-input]
[source, bash-shell]
----
kcat -b $BOOTSTRAP_SERVER -X security.protocol=SASL_SSL -X sasl.mechanisms=PLAIN -X sasl.username="$CLIENT_ID" -X sasl.password="$CLIENT_SECRET" -t songs -o 1 -C -K:

----
--
kcat in Docker::
+
--
[.console-input]
[source, bash-shell]
----
docker run -it --rm edenhill/kcat:1.7.0 kcat -b $BOOTSTRAP_SERVER -X security.protocol=SASL_SSL -X sasl.mechanisms=PLAIN -X sasl.username="$CLIENT_ID" -X sasl.password="$CLIENT_SECRET" -t songs -o 1 -C -K:
----
--
====

[.console-output]
[source, bash-shell]
----
2: {"id": 2, "name": "Still Loving you", "author":"Scorpions", "op":"ADD"}
% Reached end of topic songs [0] at offset 2
----

Now the initial offset was not set to the beginning but from the second element.

[#changingretention]
=== Changing Retention

By default, OpenShift Streams for Apache Kafka retains data on a topic for 168 hours (7 days).
But retention can be changed, so let's change it to 10 seconds.

If the browser window pointing to https://console.redhat.com[console.redhat.com] is no longer open, go to https://console.redhat.com[console.redhat.com] and log in with your Red Hat account.

On the https://console.redhat.com[console.redhat.com] landing page, select *Application Services* from the menu on the left.

On the Application Services landing page, select *Streams for Apache Kafka â†’ Kafka Instances*.

On the *Kafka Instances* page, click the name of the Kafka instance you created before.

Select the *Topics* tab, and click on the `song` topic. In the topic properties tab, click the *Edit properties* button.

Change the retention time to 10 seconds. Leave the other properties as-is.

image::rhosak-topic-properties.png[]

Scroll down to the bottom of the page and click *Save*.

In one terminal produce data again:

[tabs]
====
kcat::
+
--
[.console-input]
[source, bash-shell]
----
kcat -b $BOOTSTRAP_SERVER -X security.protocol=SASL_SSL -X sasl.mechanisms=PLAIN -X sasl.username="$CLIENT_ID" -X sasl.password="$CLIENT_SECRET" -t songs -P -l -K: initial_songs
----
--
kcat in Docker::
+
--
[.console-input]
[source, bash-shell]
----
docker run -it --rm --mount type=bind,source="$(pwd)"/initial_songs,destination=/tmp/initial_songs edenhill/kcat:1.7.0 kcat -b $BOOTSTRAP_SERVER -X security.protocol=SASL_SSL -X sasl.mechanisms=PLAIN -X sasl.username="$CLIENT_ID" -X sasl.password="$CLIENT_SECRET" -t songs -P -l -K: /tmp/initial_songs
----
--
====

In the other terminal consume this data so you know that it has been inserted correctly:

[tabs]
====
kcat::
+
--
[.console-input]
[source, bash-shell]
----
kcat -b $BOOTSTRAP_SERVER -X security.protocol=SASL_SSL -X sasl.mechanisms=PLAIN -X sasl.username="$CLIENT_ID" -X sasl.password="$CLIENT_SECRET" -t songs -C -K:
----
--
kcat in Docker::
+
--
[.console-input]
[source, bash-shell]
----
docker run -it --rm edenhill/kcat:1.7.0 kcat -b $BOOTSTRAP_SERVER -X security.protocol=SASL_SSL -X sasl.mechanisms=PLAIN -X sasl.username="$CLIENT_ID" -X sasl.password="$CLIENT_SECRET" -t songs -C -K:
----
--
====

[.console-output]
[source, bash-shell]
----
1: {"id": 1, "name": "The Ecstasy of Gold", "author":"Ennio Morricone", "op":"ADD"}
2: {"id": 2, "name": "Still Loving you", "author":"Scorpions", "op":"ADD"}
% Reached end of topic songs [0] at offset 4
----

Stop the `kcat` process by typing kbd:[Ctrl + C] and wait for ~10 seconds (ie `sleep 15`).

Run the `kcat` in consumer mode again:

[tabs]
====
kcat::
+
--
[.console-input]
[source, bash-shell]
----
kcat -b $BOOTSTRAP_SERVER -X security.protocol=SASL_SSL -X sasl.mechanisms=PLAIN -X sasl.username="$CLIENT_ID" -X sasl.password="$CLIENT_SECRET" -t songs -C -K:
----
--
kcat in Docker::
+
--
[.console-input]
[source, bash-shell]
----
docker run -it --rm edenhill/kcat:1.7.0 kcat -b $BOOTSTRAP_SERVER -X security.protocol=SASL_SSL -X sasl.mechanisms=PLAIN -X sasl.username="$CLIENT_ID" -X sasl.password="$CLIENT_SECRET" -t songs -C -K:
----
--
====

[.console-output]
[source, bash-shell]
----
% Reached end of topic songs [0] at offset 4
----

The important part is the last line, notice that it returns no messages, but the offset is `4`.
The reason is that the messages are expired and deleted from the topic, but the offset has its retention period.
By default, this retention period is 1 week, so although there is no data, the current offset for a consumer is `4`.

Reset the retention time to the default one by repeating the procedure described above to change the retention time. Set the retention time back to 7 days.
